{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Twitter analysis using SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author      : Yi-Chieh Wu, Sriram Sankararman\n",
    "Description : Twitter\n",
    "\"\"\"\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# !!! MAKE SURE TO USE SVC.decision_function(X), NOT SVC.predict(X) !!!\n",
    "# (this makes ``continuous-valued'' predictions)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "######################################################################\n",
    "# functions -- input/output\n",
    "######################################################################\n",
    "\n",
    "def read_vector_file(fname):\n",
    "    \"\"\"\n",
    "    Reads and returns a vector from a file.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        fname  -- string, filename\n",
    "        \n",
    "    Returns\n",
    "    --------------------\n",
    "        labels -- numpy array of shape (n,)\n",
    "                    n is the number of non-blank lines in the text file\n",
    "    \"\"\"\n",
    "    return np.genfromtxt(fname)\n",
    "\n",
    "\n",
    "def write_label_answer(vec, outfile):\n",
    "    \"\"\"\n",
    "    Writes your label vector to the given file.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        vec     -- numpy array of shape (n,) or (n,1), predicted scores\n",
    "        outfile -- string, output filename\n",
    "    \"\"\"\n",
    "    \n",
    "    # for this project, you should predict 70 labels\n",
    "    if(vec.shape[0] != 70):\n",
    "        print(\"Error - output vector should have 70 rows.\")\n",
    "        print(\"Aborting write.\")\n",
    "        return\n",
    "    \n",
    "    np.savetxt(outfile, vec)    \n",
    "\n",
    "\n",
    "######################################################################\n",
    "# functions -- feature extraction\n",
    "######################################################################\n",
    "\n",
    "def extract_words(input_string):\n",
    "    \"\"\"\n",
    "    Processes the input_string, separating it into \"words\" based on the presence\n",
    "    of spaces, and separating punctuation marks into their own words.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        input_string -- string of characters\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        words        -- list of lowercase \"words\"\n",
    "    \"\"\"\n",
    "    \n",
    "    for c in punctuation :\n",
    "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
    "    return input_string.lower().split()\n",
    "\n",
    "\n",
    "def extract_dictionary(infile):\n",
    "    \"\"\"\n",
    "    Given a filename, reads the text file and builds a dictionary of unique\n",
    "    words/punctuations.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile    -- string, filename\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        word_list -- dictionary, (key, value) pairs are (word, index)\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = {}\n",
    "    with open(infile, 'rU') as fid :\n",
    "        ### ========== TODO : START ========== ###\n",
    "        # part 1a: process each line to populate word_list\n",
    "        index = 0\n",
    "        lines = fid.readlines()\n",
    "        for line in lines:\n",
    "            words = extract_words(line)\n",
    "            for word in words:\n",
    "                if word not in word_list.keys():\n",
    "                    word_list[word] = index\n",
    "                    index += 1\n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def extract_feature_vectors(infile, word_list):\n",
    "    \"\"\"\n",
    "    Produces a bag-of-words representation of a text file specified by the\n",
    "    filename infile based on the dictionary word_list.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile         -- string, filename\n",
    "        word_list      -- dictionary, (key, value) pairs are (word, index)\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        feature_matrix -- numpy array of shape (n,d)\n",
    "                          boolean (0,1) array indicating word presence in a string\n",
    "                            n is the number of non-blank lines in the text file\n",
    "                            d is the number of unique words in the text file\n",
    "    \"\"\"\n",
    "    \n",
    "    num_lines = sum(1 for line in open(infile,'rU'))\n",
    "    num_words = len(word_list)\n",
    "    feature_matrix = np.zeros((num_lines, num_words))\n",
    "    \n",
    "    with open(infile, 'rU') as fid :\n",
    "        ### ========== TODO : START ========== ###\n",
    "        # part 1b: process each line to populate feature_matrix\n",
    "        lines = fid.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            words = extract_words(lines[i])\n",
    "            for j in range(len(words)):\n",
    "                feature_matrix[i][word_list[words[j]]] = 1\n",
    "        ### ========== TODO : END ========== ###\n",
    "        \n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# functions -- evaluation\n",
    "######################################################################\n",
    "\n",
    "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Calculates the performance metric based on the agreement between the \n",
    "    true labels and the predicted labels.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        y_true -- numpy array of shape (n,), known labels\n",
    "        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n",
    "        metric -- string, option used to select the performance measure\n",
    "                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n",
    "                           'sensitivity', 'specificity'        \n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score  -- float, performance score\n",
    "    \"\"\"\n",
    "    # map continuous-valued predictions to binary labels\n",
    "    y_label = np.sign(y_pred)\n",
    "    y_label[y_label==0] = 1\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2a: compute classifier performance\n",
    "    if metric == 'accuracy':\n",
    "        return metrics.accuracy_score(y_true, y_label)\n",
    "    elif metric == 'f1-score':\n",
    "        return metrics.f1_score(y_true, y_label)\n",
    "    elif metric == 'auroc':\n",
    "        return metrics.roc_auc_score(y_true, y_label)\n",
    "    elif metric == 'precision':\n",
    "        return metrics.precision_score(y_true, y_label)\n",
    "    elif metric == 'sensitivity':\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_label).ravel()\n",
    "        return tp / float(tp + fn)\n",
    "    elif metric == 'specificity':\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_label).ravel()\n",
    "        return tn / float(fp + tn)\n",
    "    else:\n",
    "        raise ValueError('invliad metric {}'.format(metric))\n",
    "\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n",
    "    Trains classifier on k-1 folds and tests on the remaining fold.\n",
    "    Calculates the k-fold cross-validation performance metric for classifier\n",
    "    by averaging the performance across folds.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf    -- classifier (instance of SVC)\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score   -- float, average cross-validation performance across k folds\n",
    "    \"\"\"\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2b: compute average cross-validation performance\n",
    "    kFold = kf(y, n_folds=5)\n",
    "    sum_score = 0\n",
    "    for train_index, test_index in kFold:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.decision_function(X_test)\n",
    "        sum_score += performance(y_test, y_pred, metric=metric)\n",
    "    return sum_score / float(5)\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def select_param_linear(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameter of a linear-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameter that 'maximize' the average k-fold CV performance.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        C -- float, optimal parameter value for linear-kernel SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Linear SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2c: select optimal hyperparameter using cross-validation\n",
    "    results = []\n",
    "    for C in C_range:\n",
    "        print('C: {}'.format(C))\n",
    "        result = cv_performance(SVC(C=C, kernel='linear'), X, y, kf=kf, metric=metric)\n",
    "        print('result: {0:.4f}\\n'.format(result))\n",
    "        results.append(result)\n",
    "    return C_range[np.argmax(results)]\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def select_param_rbf(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameters of an RBF-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameters that 'maximize' the average k-fold CV performance.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        X       -- numpy array of shape (n,d), feature vectors\n",
    "                     n = number of examples\n",
    "                     d = number of features\n",
    "        y       -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric  -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        C, gamma,  -- tuple of floats, optimal parameter values for an RBF-kernel SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    print('RBF SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 3b: create grid, then select optimal hyperparameters using cross-validation\n",
    "    # C=100, gamma=0.01 seems good\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "    gamma_range = 10.0 ** np.arange(-3, 3)\n",
    "    \n",
    "    # C=70, gamma=0.004 seems good\n",
    "    C_range = 100 + 10 * np.arange(-3, 3)\n",
    "    gamma_range = 0.01 + 0.003 * np.arange(-3, 3)\n",
    "    \n",
    "    #C=80, gamma=0.005 seems good\n",
    "    #C_range = 70 + 5 * np.arange(-3, 3)\n",
    "    #gamma_range = 0.004 + 0.001 * np.arange(-3, 3)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(len(C_range)):\n",
    "        C = C_range[i]\n",
    "        for j in range(len(gamma_range)):\n",
    "            gamma = gamma_range[j]\n",
    "            print('C: {}'.format(C))\n",
    "            print('gamma: {}'.format(gamma))\n",
    "            result = cv_performance(SVC(C=C, kernel='rbf', gamma=gamma), X, y, kf=kf, metric=metric)\n",
    "            print('result: {0:.4f}\\n'.format(result))\n",
    "            results.append(result)\n",
    "        \n",
    "    index = np.argmax(results)\n",
    "    print(index, len(gamma_range))\n",
    "    gamma_index = int(index % len(gamma_range))\n",
    "    C_index = int(index / len(gamma_range))\n",
    "    return C_range[C_index], gamma_range[gamma_index]\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def performance_test(clf, X, y, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Estimates the performance of the classifier using the 95% CI.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf          -- classifier (instance of SVC)\n",
    "                          [already fit to data]\n",
    "        X            -- numpy array of shape (n,d), feature vectors of test set\n",
    "                          n = number of examples\n",
    "                          d = number of features\n",
    "        y            -- numpy array of shape (n,), binary labels {1,-1} of test set\n",
    "        metric       -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score        -- float, classifier performance\n",
    "    \"\"\"\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 4b: return performance on test data by first computing predictions and then calling performance\n",
    "    y_pred = clf.predict(X)\n",
    "#     y_pred = clf.decision_function(X)\n",
    "#     y_label = np.sign(y_pred)\n",
    "#     y_label[y_label==0] = 1\n",
    "#     y_pred = y_label\n",
    "    \n",
    "    if metric == 'accuracy':\n",
    "        return metrics.accuracy_score(y, y_pred)\n",
    "    elif metric == 'f1-score':\n",
    "        return metrics.f1_score(y, y_pred)\n",
    "    elif metric == 'auroc':\n",
    "        return metrics.roc_auc_score(y, y_pred)\n",
    "    elif metric == 'precision':\n",
    "        return metrics.precision_score(y, y_pred)\n",
    "    elif metric == 'sensitivity':\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y, y_pred).ravel()\n",
    "        return tp / float(tp + fn)\n",
    "    elif metric == 'specificity':\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y, y_pred).ravel()\n",
    "        return tn / float(fp + tn)\n",
    "    else:\n",
    "        raise ValueError('invliad metric {}'.format(metric))\n",
    "        \n",
    "    ### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/one/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:93: DeprecationWarning: 'U' mode is deprecated\n",
      "/Users/one/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:127: DeprecationWarning: 'U' mode is deprecated\n",
      "/Users/one/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:131: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "    \n",
    "# read the tweets and its labels   \n",
    "dictionary = extract_dictionary('../data/tweets.txt')\n",
    "X = extract_feature_vectors('../data/tweets.txt', dictionary)\n",
    "y = read_vector_file('../data/labels.txt')\n",
    "    \n",
    "metric_list = [\"accuracy\", \"f1_score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part 1c: split data into training (training + cross-validation) and testing set\n",
    "X_train, X_test, y_train, y_test = X[:560], X[560:], y[:560], y[560:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 1811)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66349206349206347"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y == 1) / float(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70892857142857146"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_train == 1) / float(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29999999999999999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test == 1) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Hyperparameter Selection for a Linear-Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part 2b: create stratified folds (5-fold CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Why is it beneficial to maintain class proportions across folds?**\n",
    "\n",
    "Since we are trying to figure out the optimal hyperparameter for the training set, it makes sense to tune our hyperparameter with the folds that have same distribution as the original training set. Also, if we don’t maintain class proportions, some fold might consist of mostly positive samples, from which the model can’t really learn anything. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) finding the optimal C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Hyperparameter Selection based on accuracy:\n",
      "C: 0.001\n",
      "result: 0.7089\n",
      "\n",
      "C: 0.01\n",
      "result: 0.7107\n",
      "\n",
      "C: 0.1\n",
      "result: 0.8060\n",
      "\n",
      "C: 1.0\n",
      "result: 0.8146\n",
      "\n",
      "C: 10.0\n",
      "result: 0.8182\n",
      "\n",
      "C: 100.0\n",
      "result: 0.8182\n",
      "\n",
      "best C: 10.0\n",
      "\n",
      "Linear SVM Hyperparameter Selection based on f1-score:\n",
      "C: 0.001\n",
      "result: 0.8297\n",
      "\n",
      "C: 0.01\n",
      "result: 0.8306\n",
      "\n",
      "C: 0.1\n",
      "result: 0.8755\n",
      "\n",
      "C: 1.0\n",
      "result: 0.8749\n",
      "\n",
      "C: 10.0\n",
      "result: 0.8766\n",
      "\n",
      "C: 100.0\n",
      "result: 0.8766\n",
      "\n",
      "best C: 10.0\n",
      "\n",
      "Linear SVM Hyperparameter Selection based on auroc:\n",
      "C: 0.001\n",
      "result: 0.5000\n",
      "\n",
      "C: 0.01\n",
      "result: 0.5031\n",
      "\n",
      "C: 0.1\n",
      "result: 0.7188\n",
      "\n",
      "C: 1.0\n",
      "result: 0.7531\n",
      "\n",
      "C: 10.0\n",
      "result: 0.7592\n",
      "\n",
      "C: 100.0\n",
      "result: 0.7592\n",
      "\n",
      "best C: 10.0\n",
      "\n",
      "Linear SVM Hyperparameter Selection based on precision:\n",
      "C: 0.001\n",
      "result: 0.7089\n",
      "\n",
      "C: 0.01\n",
      "result: 0.7102\n",
      "\n",
      "C: 0.1\n",
      "result: 0.8357\n",
      "\n",
      "C: 1.0\n",
      "result: 0.8562\n",
      "\n",
      "C: 10.0\n",
      "result: 0.8595\n",
      "\n",
      "C: 100.0\n",
      "result: 0.8595\n",
      "\n",
      "best C: 10.0\n",
      "\n",
      "Linear SVM Hyperparameter Selection based on sensitivity:\n",
      "C: 0.001\n",
      "result: 1.0000\n",
      "\n",
      "C: 0.01\n",
      "result: 1.0000\n",
      "\n",
      "C: 0.1\n",
      "result: 0.9294\n",
      "\n",
      "C: 1.0\n",
      "result: 0.9017\n",
      "\n",
      "C: 10.0\n",
      "result: 0.9017\n",
      "\n",
      "C: 100.0\n",
      "result: 0.9017\n",
      "\n",
      "best C: 0.001\n",
      "\n",
      "Linear SVM Hyperparameter Selection based on specificity:\n",
      "C: 0.001\n",
      "result: 0.0000\n",
      "\n",
      "C: 0.01\n",
      "result: 0.0063\n",
      "\n",
      "C: 0.1\n",
      "result: 0.5081\n",
      "\n",
      "C: 1.0\n",
      "result: 0.6045\n",
      "\n",
      "C: 10.0\n",
      "result: 0.6167\n",
      "\n",
      "C: 100.0\n",
      "result: 0.6167\n",
      "\n",
      "best C: 10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# part 2d: for each metric, select optimal hyperparameter for linear-kernel SVM using CV\n",
    "options = ['accuracy', 'f1-score', 'auroc', 'precision', 'sensitivity', 'specificity']\n",
    "for option in options:\n",
    "    print('best C: {}\\n'.format(select_param_linear(X_train, y_train, StratifiedKFold, metric=option)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) How does the 5-fold CV performance vary with C and the performance metric?**\n",
    "\n",
    "For accuracy, F1-score, AUROC, precision, and sensitivity, the performance increases as C increases.\n",
    "\n",
    "For sensitivity, the performance decreases as C increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Hyperparameter Selection for an RBF-kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Describe the role of the additional hyperparameter γ for an RBF-kernel SVM. How does γ affect generalization error?**\n",
    "\n",
    "RBF-kernel measures the similarity between two data points. The hyperparameter γ represents the inverse of the variance of RBF-kernel. Small value of γ means large variance. In other words, even two points are not that close, they can be interpreted as close to each other. On the other hand, large value of γ means small variance. In other words, two points are considered close to each other if they are really close to each other. Therefore, the larger the γ is, the more it overfits the model because it uses only data points that are really close to each other. The smaller the γ is, the more it generalizes the model better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Explain what kind of grid you used and why.**\n",
    "\n",
    "First, I kept the same range of C from previous problem, and set the gamma to have the same range as C.\n",
    "\n",
    "* C_range = 10.0 \\*\\* np.arange(-3, 3)\n",
    "* gamma_range = 10.0 \\*\\* np.arange(-3, 3)\n",
    "\n",
    "This gives me an insight that C=100 and gamma=0.01 are good values.\n",
    "\n",
    "Next, I tried to refine C and gamma, and set the ranges as the follow.\n",
    "\n",
    "* C_range = 100 + 10 * np.arange(-3, 3)\n",
    "* gamma_range = 0.01 + 0.003 * np.arange(-3, 3)\n",
    "\n",
    "It seems C=70 and gamma=0.004 are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF SVM Hyperparameter Selection based on accuracy:\n",
      "C: 70\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8112\n",
      "\n",
      "C: 70\n",
      "gamma: 0.004\n",
      "result: 0.8164\n",
      "\n",
      "C: 70\n",
      "gamma: 0.007\n",
      "result: 0.8200\n",
      "\n",
      "C: 70\n",
      "gamma: 0.01\n",
      "result: 0.8165\n",
      "\n",
      "C: 70\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8147\n",
      "\n",
      "C: 70\n",
      "gamma: 0.016\n",
      "result: 0.8165\n",
      "\n",
      "C: 80\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8077\n",
      "\n",
      "C: 80\n",
      "gamma: 0.004\n",
      "result: 0.8200\n",
      "\n",
      "C: 80\n",
      "gamma: 0.007\n",
      "result: 0.8200\n",
      "\n",
      "C: 80\n",
      "gamma: 0.01\n",
      "result: 0.8165\n",
      "\n",
      "C: 80\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8147\n",
      "\n",
      "C: 80\n",
      "gamma: 0.016\n",
      "result: 0.8165\n",
      "\n",
      "C: 90\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8095\n",
      "\n",
      "C: 90\n",
      "gamma: 0.004\n",
      "result: 0.8182\n",
      "\n",
      "C: 90\n",
      "gamma: 0.007\n",
      "result: 0.8200\n",
      "\n",
      "C: 90\n",
      "gamma: 0.01\n",
      "result: 0.8165\n",
      "\n",
      "C: 90\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8147\n",
      "\n",
      "C: 90\n",
      "gamma: 0.016\n",
      "result: 0.8165\n",
      "\n",
      "C: 100\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8095\n",
      "\n",
      "C: 100\n",
      "gamma: 0.004\n",
      "result: 0.8200\n",
      "\n",
      "C: 100\n",
      "gamma: 0.007\n",
      "result: 0.8200\n",
      "\n",
      "C: 100\n",
      "gamma: 0.01\n",
      "result: 0.8165\n",
      "\n",
      "C: 100\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8147\n",
      "\n",
      "C: 100\n",
      "gamma: 0.016\n",
      "result: 0.8165\n",
      "\n",
      "C: 110\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8077\n",
      "\n",
      "C: 110\n",
      "gamma: 0.004\n",
      "result: 0.8182\n",
      "\n",
      "C: 110\n",
      "gamma: 0.007\n",
      "result: 0.8200\n",
      "\n",
      "C: 110\n",
      "gamma: 0.01\n",
      "result: 0.8165\n",
      "\n",
      "C: 110\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8147\n",
      "\n",
      "C: 110\n",
      "gamma: 0.016\n",
      "result: 0.8165\n",
      "\n",
      "C: 120\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8095\n",
      "\n",
      "C: 120\n",
      "gamma: 0.004\n",
      "result: 0.8182\n",
      "\n",
      "C: 120\n",
      "gamma: 0.007\n",
      "result: 0.8200\n",
      "\n",
      "C: 120\n",
      "gamma: 0.01\n",
      "result: 0.8165\n",
      "\n",
      "C: 120\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8147\n",
      "\n",
      "C: 120\n",
      "gamma: 0.016\n",
      "result: 0.8165\n",
      "\n",
      "2 6\n",
      "best (C, gamma): (70, 0.0070000000000000001)\n",
      "\n",
      "RBF SVM Hyperparameter Selection based on f1-score:\n",
      "C: 70\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8767\n",
      "\n",
      "C: 70\n",
      "gamma: 0.004\n",
      "result: 0.8755\n",
      "\n",
      "C: 70\n",
      "gamma: 0.007\n",
      "result: 0.8786\n",
      "\n",
      "C: 70\n",
      "gamma: 0.01\n",
      "result: 0.8763\n",
      "\n",
      "C: 70\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8750\n",
      "\n",
      "C: 70\n",
      "gamma: 0.016\n",
      "result: 0.8765\n",
      "\n",
      "C: 80\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8738\n",
      "\n",
      "C: 80\n",
      "gamma: 0.004\n",
      "result: 0.8789\n",
      "\n",
      "C: 80\n",
      "gamma: 0.007\n",
      "result: 0.8786\n",
      "\n",
      "C: 80\n",
      "gamma: 0.01\n",
      "result: 0.8763\n",
      "\n",
      "C: 80\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8750\n",
      "\n",
      "C: 80\n",
      "gamma: 0.016\n",
      "result: 0.8765\n",
      "\n",
      "C: 90\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8737\n",
      "\n",
      "C: 90\n",
      "gamma: 0.004\n",
      "result: 0.8776\n",
      "\n",
      "C: 90\n",
      "gamma: 0.007\n",
      "result: 0.8786\n",
      "\n",
      "C: 90\n",
      "gamma: 0.01\n",
      "result: 0.8763\n",
      "\n",
      "C: 90\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8750\n",
      "\n",
      "C: 90\n",
      "gamma: 0.016\n",
      "result: 0.8765\n",
      "\n",
      "C: 100\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8733\n",
      "\n",
      "C: 100\n",
      "gamma: 0.004\n",
      "result: 0.8790\n",
      "\n",
      "C: 100\n",
      "gamma: 0.007\n",
      "result: 0.8786\n",
      "\n",
      "C: 100\n",
      "gamma: 0.01\n",
      "result: 0.8763\n",
      "\n",
      "C: 100\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8750\n",
      "\n",
      "C: 100\n",
      "gamma: 0.016\n",
      "result: 0.8765\n",
      "\n",
      "C: 110\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8719\n",
      "\n",
      "C: 110\n",
      "gamma: 0.004\n",
      "result: 0.8776\n",
      "\n",
      "C: 110\n",
      "gamma: 0.007\n",
      "result: 0.8786\n",
      "\n",
      "C: 110\n",
      "gamma: 0.01\n",
      "result: 0.8763\n",
      "\n",
      "C: 110\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8750\n",
      "\n",
      "C: 110\n",
      "gamma: 0.016\n",
      "result: 0.8765\n",
      "\n",
      "C: 120\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8723\n",
      "\n",
      "C: 120\n",
      "gamma: 0.004\n",
      "result: 0.8776\n",
      "\n",
      "C: 120\n",
      "gamma: 0.007\n",
      "result: 0.8786\n",
      "\n",
      "C: 120\n",
      "gamma: 0.01\n",
      "result: 0.8763\n",
      "\n",
      "C: 120\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8750\n",
      "\n",
      "C: 120\n",
      "gamma: 0.016\n",
      "result: 0.8765\n",
      "\n",
      "19 6\n",
      "best (C, gamma): (100, 0.0040000000000000001)\n",
      "\n",
      "RBF SVM Hyperparameter Selection based on auroc:\n",
      "C: 70\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.7331\n",
      "\n",
      "C: 70\n",
      "gamma: 0.004\n",
      "result: 0.7580\n",
      "\n",
      "C: 70\n",
      "gamma: 0.007\n",
      "result: 0.7588\n",
      "\n",
      "C: 70\n",
      "gamma: 0.01\n",
      "result: 0.7545\n",
      "\n",
      "C: 70\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.7514\n",
      "\n",
      "C: 70\n",
      "gamma: 0.016\n",
      "result: 0.7526\n",
      "\n",
      "C: 80\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.7324\n",
      "\n",
      "C: 80\n",
      "gamma: 0.004\n",
      "result: 0.7588\n",
      "\n",
      "C: 80\n",
      "gamma: 0.007\n",
      "result: 0.7588\n",
      "\n",
      "C: 80\n",
      "gamma: 0.01\n",
      "result: 0.7545\n",
      "\n",
      "C: 80\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.7514\n",
      "\n",
      "C: 80\n",
      "gamma: 0.016\n",
      "result: 0.7526\n",
      "\n",
      "C: 90\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.7390\n",
      "\n",
      "C: 90\n",
      "gamma: 0.004\n",
      "result: 0.7575\n",
      "\n",
      "C: 90\n",
      "gamma: 0.007\n",
      "result: 0.7588\n",
      "\n",
      "C: 90\n",
      "gamma: 0.01\n",
      "result: 0.7545\n",
      "\n",
      "C: 90\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.7514\n",
      "\n",
      "C: 90\n",
      "gamma: 0.016\n",
      "result: 0.7526\n",
      "\n",
      "C: 100\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.7408\n",
      "\n",
      "C: 100\n",
      "gamma: 0.004\n",
      "result: 0.7588\n",
      "\n",
      "C: 100\n",
      "gamma: 0.007\n",
      "result: 0.7588\n",
      "\n",
      "C: 100\n",
      "gamma: 0.01\n",
      "result: 0.7545\n",
      "\n",
      "C: 100\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.7514\n",
      "\n",
      "C: 100\n",
      "gamma: 0.016\n",
      "result: 0.7526\n",
      "\n",
      "C: 110\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.7395\n",
      "\n",
      "C: 110\n",
      "gamma: 0.004\n",
      "result: 0.7556\n",
      "\n",
      "C: 110\n",
      "gamma: 0.007\n",
      "result: 0.7588\n",
      "\n",
      "C: 110\n",
      "gamma: 0.01\n",
      "result: 0.7545\n",
      "\n",
      "C: 110\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.7514\n",
      "\n",
      "C: 110\n",
      "gamma: 0.016\n",
      "result: 0.7526\n",
      "\n",
      "C: 120\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.7443\n",
      "\n",
      "C: 120\n",
      "gamma: 0.004\n",
      "result: 0.7556\n",
      "\n",
      "C: 120\n",
      "gamma: 0.007\n",
      "result: 0.7588\n",
      "\n",
      "C: 120\n",
      "gamma: 0.01\n",
      "result: 0.7545\n",
      "\n",
      "C: 120\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.7514\n",
      "\n",
      "C: 120\n",
      "gamma: 0.016\n",
      "result: 0.7526\n",
      "\n",
      "2 6\n",
      "best (C, gamma): (70, 0.0070000000000000001)\n",
      "\n",
      "RBF SVM Hyperparameter Selection based on precision:\n",
      "C: 70\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8428\n",
      "\n",
      "C: 70\n",
      "gamma: 0.004\n",
      "result: 0.8610\n",
      "\n",
      "C: 70\n",
      "gamma: 0.007\n",
      "result: 0.8593\n",
      "\n",
      "C: 70\n",
      "gamma: 0.01\n",
      "result: 0.8583\n",
      "\n",
      "C: 70\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8560\n",
      "\n",
      "C: 70\n",
      "gamma: 0.016\n",
      "result: 0.8560\n",
      "\n",
      "C: 80\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8440\n",
      "\n",
      "C: 80\n",
      "gamma: 0.004\n",
      "result: 0.8607\n",
      "\n",
      "C: 80\n",
      "gamma: 0.007\n",
      "result: 0.8593\n",
      "\n",
      "C: 80\n",
      "gamma: 0.01\n",
      "result: 0.8583\n",
      "\n",
      "C: 80\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8560\n",
      "\n",
      "C: 80\n",
      "gamma: 0.016\n",
      "result: 0.8560\n",
      "\n",
      "C: 90\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8482\n",
      "\n",
      "C: 90\n",
      "gamma: 0.004\n",
      "result: 0.8603\n",
      "\n",
      "C: 90\n",
      "gamma: 0.007\n",
      "result: 0.8593\n",
      "\n",
      "C: 90\n",
      "gamma: 0.01\n",
      "result: 0.8583\n",
      "\n",
      "C: 90\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8560\n",
      "\n",
      "C: 90\n",
      "gamma: 0.016\n",
      "result: 0.8560\n",
      "\n",
      "C: 100\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8499\n",
      "\n",
      "C: 100\n",
      "gamma: 0.004\n",
      "result: 0.8606\n",
      "\n",
      "C: 100\n",
      "gamma: 0.007\n",
      "result: 0.8593\n",
      "\n",
      "C: 100\n",
      "gamma: 0.01\n",
      "result: 0.8583\n",
      "\n",
      "C: 100\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8560\n",
      "\n",
      "C: 100\n",
      "gamma: 0.016\n",
      "result: 0.8560\n",
      "\n",
      "C: 110\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8493\n",
      "\n",
      "C: 110\n",
      "gamma: 0.004\n",
      "result: 0.8570\n",
      "\n",
      "C: 110\n",
      "gamma: 0.007\n",
      "result: 0.8593\n",
      "\n",
      "C: 110\n",
      "gamma: 0.01\n",
      "result: 0.8583\n",
      "\n",
      "C: 110\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8560\n",
      "\n",
      "C: 110\n",
      "gamma: 0.016\n",
      "result: 0.8560\n",
      "\n",
      "C: 120\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.8516\n",
      "\n",
      "C: 120\n",
      "gamma: 0.004\n",
      "result: 0.8570\n",
      "\n",
      "C: 120\n",
      "gamma: 0.007\n",
      "result: 0.8593\n",
      "\n",
      "C: 120\n",
      "gamma: 0.01\n",
      "result: 0.8583\n",
      "\n",
      "C: 120\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.8560\n",
      "\n",
      "C: 120\n",
      "gamma: 0.016\n",
      "result: 0.8560\n",
      "\n",
      "1 6\n",
      "best (C, gamma): (70, 0.0040000000000000001)\n",
      "\n",
      "RBF SVM Hyperparameter Selection based on sensitivity:\n",
      "C: 70\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.9218\n",
      "\n",
      "C: 70\n",
      "gamma: 0.004\n",
      "result: 0.8992\n",
      "\n",
      "C: 70\n",
      "gamma: 0.007\n",
      "result: 0.9067\n",
      "\n",
      "C: 70\n",
      "gamma: 0.01\n",
      "result: 0.9042\n",
      "\n",
      "C: 70\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.9042\n",
      "\n",
      "C: 70\n",
      "gamma: 0.016\n",
      "result: 0.9068\n",
      "\n",
      "C: 80\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.9143\n",
      "\n",
      "C: 80\n",
      "gamma: 0.004\n",
      "result: 0.9067\n",
      "\n",
      "C: 80\n",
      "gamma: 0.007\n",
      "result: 0.9067\n",
      "\n",
      "C: 80\n",
      "gamma: 0.01\n",
      "result: 0.9042\n",
      "\n",
      "C: 80\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.9042\n",
      "\n",
      "C: 80\n",
      "gamma: 0.016\n",
      "result: 0.9068\n",
      "\n",
      "C: 90\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.9093\n",
      "\n",
      "C: 90\n",
      "gamma: 0.004\n",
      "result: 0.9042\n",
      "\n",
      "C: 90\n",
      "gamma: 0.007\n",
      "result: 0.9067\n",
      "\n",
      "C: 90\n",
      "gamma: 0.01\n",
      "result: 0.9042\n",
      "\n",
      "C: 90\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.9042\n",
      "\n",
      "C: 90\n",
      "gamma: 0.016\n",
      "result: 0.9068\n",
      "\n",
      "C: 100\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.9068\n",
      "\n",
      "C: 100\n",
      "gamma: 0.004\n",
      "result: 0.9067\n",
      "\n",
      "C: 100\n",
      "gamma: 0.007\n",
      "result: 0.9067\n",
      "\n",
      "C: 100\n",
      "gamma: 0.01\n",
      "result: 0.9042\n",
      "\n",
      "C: 100\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.9042\n",
      "\n",
      "C: 100\n",
      "gamma: 0.016\n",
      "result: 0.9068\n",
      "\n",
      "C: 110\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.9042\n",
      "\n",
      "C: 110\n",
      "gamma: 0.004\n",
      "result: 0.9067\n",
      "\n",
      "C: 110\n",
      "gamma: 0.007\n",
      "result: 0.9067\n",
      "\n",
      "C: 110\n",
      "gamma: 0.01\n",
      "result: 0.9042\n",
      "\n",
      "C: 110\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.9042\n",
      "\n",
      "C: 110\n",
      "gamma: 0.016\n",
      "result: 0.9068\n",
      "\n",
      "C: 120\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.9017\n",
      "\n",
      "C: 120\n",
      "gamma: 0.004\n",
      "result: 0.9067\n",
      "\n",
      "C: 120\n",
      "gamma: 0.007\n",
      "result: 0.9067\n",
      "\n",
      "C: 120\n",
      "gamma: 0.01\n",
      "result: 0.9042\n",
      "\n",
      "C: 120\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.9042\n",
      "\n",
      "C: 120\n",
      "gamma: 0.016\n",
      "result: 0.9068\n",
      "\n",
      "0 6\n",
      "best (C, gamma): (70, 0.00099999999999999915)\n",
      "\n",
      "RBF SVM Hyperparameter Selection based on specificity:\n",
      "C: 70\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.5443\n",
      "\n",
      "C: 70\n",
      "gamma: 0.004\n",
      "result: 0.6169\n",
      "\n",
      "C: 70\n",
      "gamma: 0.007\n",
      "result: 0.6108\n",
      "\n",
      "C: 70\n",
      "gamma: 0.01\n",
      "result: 0.6047\n",
      "\n",
      "C: 70\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.5985\n",
      "\n",
      "C: 70\n",
      "gamma: 0.016\n",
      "result: 0.5985\n",
      "\n",
      "C: 80\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.5506\n",
      "\n",
      "C: 80\n",
      "gamma: 0.004\n",
      "result: 0.6108\n",
      "\n",
      "C: 80\n",
      "gamma: 0.007\n",
      "result: 0.6108\n",
      "\n",
      "C: 80\n",
      "gamma: 0.01\n",
      "result: 0.6047\n",
      "\n",
      "C: 80\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.5985\n",
      "\n",
      "C: 80\n",
      "gamma: 0.016\n",
      "result: 0.5985\n",
      "\n",
      "C: 90\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.5687\n",
      "\n",
      "C: 90\n",
      "gamma: 0.004\n",
      "result: 0.6108\n",
      "\n",
      "C: 90\n",
      "gamma: 0.007\n",
      "result: 0.6108\n",
      "\n",
      "C: 90\n",
      "gamma: 0.01\n",
      "result: 0.6047\n",
      "\n",
      "C: 90\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.5985\n",
      "\n",
      "C: 90\n",
      "gamma: 0.016\n",
      "result: 0.5985\n",
      "\n",
      "C: 100\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.5748\n",
      "\n",
      "C: 100\n",
      "gamma: 0.004\n",
      "result: 0.6108\n",
      "\n",
      "C: 100\n",
      "gamma: 0.007\n",
      "result: 0.6108\n",
      "\n",
      "C: 100\n",
      "gamma: 0.01\n",
      "result: 0.6047\n",
      "\n",
      "C: 100\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.5985\n",
      "\n",
      "C: 100\n",
      "gamma: 0.016\n",
      "result: 0.5985\n",
      "\n",
      "C: 110\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.5748\n",
      "\n",
      "C: 110\n",
      "gamma: 0.004\n",
      "result: 0.6045\n",
      "\n",
      "C: 110\n",
      "gamma: 0.007\n",
      "result: 0.6108\n",
      "\n",
      "C: 110\n",
      "gamma: 0.01\n",
      "result: 0.6047\n",
      "\n",
      "C: 110\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.5985\n",
      "\n",
      "C: 110\n",
      "gamma: 0.016\n",
      "result: 0.5985\n",
      "\n",
      "C: 120\n",
      "gamma: 0.0009999999999999992\n",
      "result: 0.5869\n",
      "\n",
      "C: 120\n",
      "gamma: 0.004\n",
      "result: 0.6045\n",
      "\n",
      "C: 120\n",
      "gamma: 0.007\n",
      "result: 0.6108\n",
      "\n",
      "C: 120\n",
      "gamma: 0.01\n",
      "result: 0.6047\n",
      "\n",
      "C: 120\n",
      "gamma: 0.013000000000000001\n",
      "result: 0.5985\n",
      "\n",
      "C: 120\n",
      "gamma: 0.016\n",
      "result: 0.5985\n",
      "\n",
      "1 6\n",
      "best (C, gamma): (70, 0.0040000000000000001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# part 3c: for each metric, select optimal hyperparameter for RBF-SVM using CV\n",
    "options = ['accuracy', 'f1-score', 'auroc', 'precision', 'sensitivity', 'specificity']\n",
    "for option in options:\n",
    "    print('best (C, gamma): {}\\n'.format(select_param_rbf(X_train, y_train, StratifiedKFold, metric=option)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) report only the best score for each metric, along with the accompanying C and γ setting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) How does the CV performance vary with the hyperparameters of the RBF-kernel SVM?**\n",
    "\n",
    "The performance increases as C increases. The performance increases as gamma decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Test Set Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) explains your choice**\n",
    "\n",
    "I chose C=10 for linear kernel because most of the metrics perform the best when C=10.\n",
    "\n",
    "I chose C=80 and gamma=0.005 for rbf kernel because most of the metrics perform the best when C=80 and gamma=0.005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training linear SVM...\n",
      "training RBF-kernel SVM...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=70, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.004, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part 4a: train linear- and RBF-kernel SVMs with selected hyperparameters\n",
    "print('training linear SVM...')\n",
    "clf_linear = SVC(C=10, kernel='linear')\n",
    "clf_linear.fit(X_train, y_train)\n",
    "\n",
    "print('training RBF-kernel SVM...')\n",
    "clf_rbf = SVC(C=70, gamma=0.004, kernel='rbf')\n",
    "clf_rbf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing linear SVM...\n",
      "accuracy: 0.7429\n",
      "f1-score: 0.4375\n",
      "auroc: 0.6259\n",
      "precision: 0.6364\n",
      "sensitivity: 0.3333\n",
      "specificity: 0.9184\n",
      "\n",
      "Testing RBF-kernel SVM...\n",
      "accuracy: 0.7571\n",
      "f1-score: 0.4516\n",
      "auroc: 0.6361\n",
      "precision: 0.7000\n",
      "sensitivity: 0.3333\n",
      "specificity: 0.9388\n"
     ]
    }
   ],
   "source": [
    "# part 4c: report performance on test data\n",
    "options = ['accuracy', 'f1-score', 'auroc', 'precision', 'sensitivity', 'specificity']\n",
    "print('Testing linear SVM...')\n",
    "for option in options:\n",
    "    print('{0}: {1:.4f}'.format(option, performance_test(clf_linear, X_test, y_test, metric=option)))\n",
    "\n",
    "print('\\nTesting RBF-kernel SVM...')\n",
    "for option in options:\n",
    "    print('{0}: {1:.4f}'.format(option, performance_test(clf_rbf, X_test, y_test, metric=option)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(c) How do the test performance of your two classifiers compare?**\n",
    "\n",
    "Testing linear SVM...\n",
    "* accuracy: 0.7429\n",
    "* f1-score: 0.4375\n",
    "* auroc: 0.6259\n",
    "* precision: 0.6364\n",
    "* sensitivity: 0.3333\n",
    "* specificity: 0.9184\n",
    "\n",
    "Testing RBF-kernel SVM...\n",
    "* accuracy: 0.7571\n",
    "* f1-score: 0.4516\n",
    "* auroc: 0.6361\n",
    "* precision: 0.7000\n",
    "* sensitivity: 0.3333\n",
    "* specificity: 0.9388\n",
    "\n",
    "It seems both SVMs have the same performance. My guess is that since the shape of X_train is (560, 1811), which means that number of features is much larger than number of training examples, increasing the complexity of the model does not help the performance. But in general, where there are way more training examples than features, RBF-kernel should perform better because it allows the model to be more complex than linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}